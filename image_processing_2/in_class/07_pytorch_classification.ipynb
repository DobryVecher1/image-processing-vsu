{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pfcsyrDnlYn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Hwyb51n7gH"
      },
      "outputs": [],
      "source": [
        "class DatasetCIFAR(Dataset):\n",
        "\n",
        "    def __init__(self, x_data, y_data, transform=None):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Load and return a sample from the dataset at the given index.\"\"\"\n",
        "        img = self.x_data[index]\n",
        "\n",
        "        # augmentations\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = torch.from_numpy(self.y_data[index])\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of samples in dataset.\"\"\"\n",
        "        return len(self.x_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_706Y1Or2no"
      },
      "outputs": [],
      "source": [
        "class DatamoduleCIFAR():\n",
        "    \"\"\"Create dataset and loaders, apply transforms.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # load data\n",
        "        trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        valset = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "        self.x_train = np.stack([np.array(img)/255. for img, _ in trainset])  # (N, H, W, C)\n",
        "        self.y_train = np.array([label for _, label in trainset], dtype=int)[:, None]\n",
        "\n",
        "        self.x_test = np.stack([np.array(img)/255. for img, _ in valset])\n",
        "        self.y_test = np.array([label for _, label in valset], dtype=int)[:, None]\n",
        "\n",
        "    def create_loaders(self):\n",
        "        \"\"\"Create loaders both for train and test/validation datasets.\"\"\"\n",
        "        # train dataset\n",
        "        dset_train = DatasetCIFAR(self.x_train, self.y_train, transform=transforms.ToTensor())\n",
        "        # test dataset\n",
        "        dset_test = DatasetCIFAR(self.x_test, self.y_test, transform=transforms.ToTensor())\n",
        "\n",
        "        # Train and test dataloaders\n",
        "        train_loader = DataLoader(dset_train, batch_size=100, shuffle=True)\n",
        "        test_loader = DataLoader(dset_test, batch_size=100, shuffle=False)\n",
        "\n",
        "        return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjBKBe1Fva0c"
      },
      "outputs": [],
      "source": [
        "class ModelCIFAR(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=4 * 4 * 16, out_features=10)\n",
        "        )\n",
        "\n",
        "        self.loss_ce = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(self.cnn.parameters(), lr=1e-3)\n",
        "\n",
        "        # Metrics\n",
        "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
        "        self.prec = torchmetrics.Precision(task='multiclass', num_classes=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnn(x)\n",
        "\n",
        "    def fit(self, train_loader, test_loader, num_epoch=50):\n",
        "\n",
        "        for ii in range(num_epoch):\n",
        "\n",
        "            loss_batches = []\n",
        "            preds_train = []\n",
        "            labels_train = []\n",
        "            # train\n",
        "            for step, (images, labels) in enumerate(train_loader):\n",
        "                # to cuda\n",
        "                # images = images #.cuda()\n",
        "                # labels = labels #.cuda()\n",
        "                self.cnn.train()\n",
        "                # make prediction\n",
        "                logits_cls = self.cnn(images.float())\n",
        "\n",
        "                # calculate loss\n",
        "                loss = self.loss_ce(logits_cls, labels[:, 0])\n",
        "\n",
        "                # update weights\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # save loss\n",
        "                loss_batches.append(loss.item())\n",
        "\n",
        "                # predictions\n",
        "                labels_pred = torch.argmax(nn.Softmax(dim=1)(logits_cls), dim=1)\n",
        "\n",
        "                preds_train.append(labels_pred)\n",
        "                labels_train.append(labels[:, 0])\n",
        "\n",
        "\n",
        "            # find metrics in the end of the epoch\n",
        "            predictions = torch.cat([preds for preds in preds_train])\n",
        "            labels = torch.cat([labels for labels in labels_train])\n",
        "\n",
        "            acc_train = self.accuracy(predictions, labels)\n",
        "            prec_train = self.prec(predictions, labels)\n",
        "\n",
        "            print(f\"Epoch: {ii}\")\n",
        "            print(f\"TRAIN | Loss: {np.mean(loss_batches): .3f}, Train_acc: {acc_train: .3f}, Train_prec: {prec_train: .3f}\")\n",
        "\n",
        "            # test\n",
        "            with torch.no_grad():\n",
        "                loss_batches_test = []\n",
        "                preds_test = []\n",
        "                labels_test = []\n",
        "                for step, (images, labels) in enumerate(test_loader):\n",
        "                    # images = images #.cuda()\n",
        "                    # labels = labels #.cuda()\n",
        "                    \n",
        "                    self.cnn.eval()\n",
        "                    # logits_cls = self.forward(images)\n",
        "                    logits_cls = self.cnn(images.float())\n",
        "\n",
        "                    loss = self.loss_ce(logits_cls, labels[:, 0])\n",
        "\n",
        "                    # save loss\n",
        "                    loss_batches_test.append(loss.item())\n",
        "\n",
        "                    # predictions\n",
        "                    labels_pred = torch.argmax(nn.Softmax(dim=1)(logits_cls), dim=1)\n",
        "\n",
        "                    preds_test.append(labels_pred)\n",
        "                    labels_test.append(labels[:, 0])\n",
        "\n",
        "                # find metrics in the end of the epoch\n",
        "                predictions = torch.cat([preds for preds in preds_test])\n",
        "                labels = torch.cat([labels for labels in labels_test])\n",
        "\n",
        "                acc_test = self.accuracy(predictions, labels)\n",
        "                prec_test = self.prec(predictions, labels)\n",
        "\n",
        "                print(f\"TEST | Loss: {np.mean(loss_batches_test): .3f}, Test_acc: {acc_test: .3f}, Test_prec: {prec_test: .3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRj4BzF64OPg"
      },
      "outputs": [],
      "source": [
        "cnn_model = ModelCIFAR() # .cuda()\n",
        "\n",
        "train_loader, test_loader = DatamoduleCIFAR().create_loaders()\n",
        "\n",
        "cnn_model.fit(train_loader, test_loader, num_epoch=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJVhYujq4XU3"
      },
      "outputs": [],
      "source": [
        "# Количество обучаемых параметров\n",
        "sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPPr3hD/e8EKBRLlCH/8yPK",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
